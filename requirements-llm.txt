transformers>=4.0.0
torch>=1.12.0
requests
# Optional: accelerate for faster local model inference on multi-GPU
accelerate
